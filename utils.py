import numpy as np
import torch
from torch.optim import Adam
from tqdm import tqdm
import pickle
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
import os

def train(
    model,
    config,
    train_loader,
    valid_loader=None,
    valid_epoch_interval=20,
    foldername="",
):
    """
    è®­ç»ƒå‡½æ•°ï¼Œé™„å¸¦ä¸­æ–‡æ‰“å°ï¼š
      - æ¯ä¸ª epoch çš„å¼€å§‹å’Œç»“æŸæŸå¤±
      - å‰ä¸¤æ‰¹æ ·æœ¬çš„æ—¥æœŸå’Œ mask ä¿¡æ¯
      - å­¦ä¹ ç‡è°ƒåº¦æƒ…å†µ
    Args:
      model         : å¾…è®­ç»ƒçš„ CSDI æ¨¡å‹
      config        : é…ç½®å­—å…¸ï¼Œéœ€åŒ…å« config["train"]["lr"], config["train"]["epochs"], config["train"]["itr_per_epoch"]
      train_loader  : è®­ç»ƒé›† DataLoaderï¼Œbatch ä¸­å« 'observed_data','observed_mask','gt_mask','timepoints'ï¼Œå¯é€‰ 'dates'
      valid_loader  : éªŒè¯é›† DataLoaderï¼Œå’Œ train_loader åŒç»“æ„
      valid_epoch_interval : æ¯éš”å¤šå°‘ epoch åšä¸€æ¬¡éªŒè¯
      foldername    : è‹¥éç©ºï¼Œåˆ™åœ¨æ­¤ç›®å½•ä¸‹ä¿å­˜æœ€ä½³æ¨¡å‹å’Œ loss æ›²çº¿
    """
    # ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = Adam(model.parameters(), lr=config["train"]["lr"], weight_decay=1e-6)
    if foldername:
        os.makedirs(foldername, exist_ok=True)
        output_path = os.path.join(foldername, "model.pth")

    p1 = int(0.75 * config["train"]["epochs"])
    p2 = int(0.9 * config["train"]["epochs"])
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[p1, p2], gamma=0.1)

    best_valid_loss = float('inf')
    train_losses, valid_losses = [], []

    for epoch_no in range(config["train"]["epochs"]):
        model.train()
        epoch_loss = 0.0
        print(f"\nğŸ“… Epoch {epoch_no+1}/{config['train']['epochs']} å¼€å§‹è®­ç»ƒï¼Œå½“å‰å­¦ä¹ ç‡ï¼š{optimizer.param_groups[0]['lr']:.6f}")
        with tqdm(train_loader, desc=f"Epoch {epoch_no}") as it:
            for batch_no, batch in enumerate(it, start=1):
                # å‰ä¸¤æ‰¹æ‰“å°æ—¥æœŸå’Œ mask æ•°é‡
                if batch_no <= 2 and "dates" in batch:
                    for b in range(len(batch["dates"])):
                        mcnt = int((batch["gt_mask"][b,:,0]==0).sum().item())
                        print(f"  [Train Batch{batch_no} Sample{b}] Date={batch['dates'][b]}, mask_count={mcnt}")

                optimizer.zero_grad()
                loss = model(batch)    # forward è¿”å› loss
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                it.set_postfix(avg_loss=epoch_loss / batch_no)
                if batch_no >= config["train"]["itr_per_epoch"]:
                    break

        avg_train_loss = epoch_loss / batch_no
        train_losses.append(avg_train_loss)
        print(f"âœ… Epoch {epoch_no+1} è®­ç»ƒå®Œæ¯•ï¼Œå¹³å‡ Loss={avg_train_loss:.4f}")
        lr_scheduler.step()

        # éªŒè¯
        if valid_loader and (epoch_no+1) % valid_epoch_interval == 0:
            model.eval()
            valid_sum = 0.0
            print(f"ğŸ” å¼€å§‹ç¬¬ {epoch_no+1} ä¸ª epoch çš„éªŒè¯")
            with torch.no_grad(), tqdm(valid_loader, desc=f"Valid {epoch_no}") as itv:
                for vb, vbatch in enumerate(itv, start=1):
                    vloss = model(vbatch, is_train=0)
                    valid_sum += vloss.item()
                    itv.set_postfix(valid_loss=valid_sum / vb)
            avg_valid_loss = valid_sum / vb
            valid_losses.append(avg_valid_loss)
            print(f"âœ… éªŒè¯å®Œæ¯•ï¼Œå¹³å‡ Valid Loss={avg_valid_loss:.4f}")
            # ä¿å­˜æœ€ä½³
            if avg_valid_loss < best_valid_loss:
                best_valid_loss = avg_valid_loss
                if foldername:
                    torch.save(model.state_dict(), output_path)
                    print(f"ğŸ’¾ New best model saved at epoch {epoch_no+1}, loss={avg_valid_loss:.4f}")

        # ç»˜åˆ¶å¹¶ä¿å­˜ loss æ›²çº¿
        plt.figure()
        plt.plot(range(1, len(train_losses)+1), train_losses, label="Train")
        if valid_losses:
            x_valid = list(range(valid_epoch_interval, valid_epoch_interval*len(valid_losses)+1, valid_epoch_interval))
            plt.plot(x_valid, valid_losses, label="Valid")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)
        if foldername:
            curve_path = os.path.join(foldername, "loss_curve.png")
            plt.savefig(curve_path)
            print(f"ğŸ“ˆ Loss curve saved to {curve_path}")
        plt.close()
    


def quantile_loss(target, forecast, q: float, eval_points) -> float:
    return 2 * torch.sum(
        torch.abs((forecast - target) * eval_points * ((target <= forecast) * 1.0 - q))
    )


def calc_denominator(target, eval_points):
    return torch.sum(torch.abs(target * eval_points))
def r_squared(y_true, y_pred, eval_points):
    y_true = y_true * eval_points
    y_pred = y_pred * eval_points
    mean_y = torch.sum(y_true) / torch.sum(eval_points)
    ss_total = torch.sum((y_true - mean_y) ** 2)
    ss_res = torch.sum((y_true - y_pred) ** 2)
    return 1 - ss_res / ss_total

def calc_quantile_CRPS(target, forecast, eval_points, mean_scaler, scaler):

    target = target * scaler + mean_scaler
    forecast = forecast * scaler + mean_scaler

    quantiles = np.arange(0.05, 1.0, 0.05)
    denom = calc_denominator(target, eval_points)
    CRPS = 0
    for i in range(len(quantiles)):
        q_pred = []
        for j in range(len(forecast)):
            q_pred.append(torch.quantile(forecast[j : j + 1], quantiles[i], dim=1))
        q_pred = torch.cat(q_pred, 0)
        q_loss = quantile_loss(target, q_pred, quantiles[i], eval_points)
        CRPS += q_loss / denom
    return CRPS.item() / len(quantiles)

def calc_quantile_CRPS_sum(target, forecast, eval_points, mean_scaler, scaler):

    eval_points = eval_points.mean(-1)
    target = target * scaler + mean_scaler
    target = target.sum(-1)
    forecast = forecast * scaler + mean_scaler

    quantiles = np.arange(0.05, 1.0, 0.05)
    denom = calc_denominator(target, eval_points)
    CRPS = 0
    for i in range(len(quantiles)):
        q_pred = torch.quantile(forecast.sum(-1),quantiles[i],dim=1)
        q_loss = quantile_loss(target, q_pred, quantiles[i], eval_points)
        CRPS += q_loss / denom
    return CRPS.item() / len(quantiles)
import os, pickle, numpy as np
import torch
from tqdm import tqdm
from sklearn.metrics import r2_score
def evaluate(
    model,
    test_loader,
    nsample=100,
    scaler=1.0,
    mean_scaler=0.0,
    foldername="",
):
    """
    è¯„ä¼°å‡½æ•°ï¼Œè¾“å‡º RMSE/MAE/RÂ²/MAPEï¼Œå¹¶å¯ä¿å­˜ç»“æœä¸ä¸­é—´æ ·æœ¬ã€‚
    Args:
      model        : å·²è®­ç»ƒå¥½çš„ CSDI_Speed æ¨¡å‹
      test_loader  : æµ‹è¯•é›† DataLoaderï¼Œbatch ä¸­éœ€å« 'observed_data','observed_mask','gt_mask','timepoints'ï¼Œå¯é€‰ 'raw_data','date'
      nsample      : æ¯æ¡åºåˆ—ç”Ÿæˆæ ·æœ¬æ•°
      scaler       : åå½’ä¸€åŒ–ç”¨æ ‡å‡†å·®ï¼ˆscalarï¼‰
      mean_scaler  : åå½’ä¸€åŒ–ç”¨å‡å€¼ï¼ˆscalarï¼‰
      foldername   : éç©ºæ—¶ä¿å­˜ metrics.txt + generated_outputs_nsample{nsample}.pk + result_nsample{nsample}.pk
    """
    device = model.device
    print(f"\nğŸ§ª Starting evaluation on {len(test_loader.dataset)} samples")
    print(f"   â†’ ä½¿ç”¨ scaler={scaler:.6f}, mean={mean_scaler:.6f} åå½’ä¸€åŒ–\n")

    all_preds, all_trues = [], []
    all_generated_samples = []
    all_target = []
    all_evalpoint = []
    all_observed_time = []

    mse_sum = mae_sum = total_pts = 0

    model.eval()
    with torch.no_grad():
        for batch_i, batch in enumerate(tqdm(test_loader, desc="Evaluating"), start=1):
            batch_gpu = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}

            gm = batch_gpu["gt_mask"].float()
            eval_mask = (gm == 0).float()
            eval_mask[:, :, 1:] = 0.0  # åªè¯„ä¼° speed é€šé“
            mask_np = eval_mask.cpu().numpy().squeeze(-1).astype(bool)

            pts = int(mask_np.sum())
            if pts == 0:
                print(f"âš ï¸  Batch {batch_i} æ²¡æœ‰ eval ç‚¹ï¼Œè·³è¿‡")
                continue

            samples, full_seq, _, _, _ = model.evaluate(batch, nsample)
            samples = samples.permute(0, 1, 3, 2)  # (B, S, T, D)
            samples_np = samples.cpu().numpy()
            full_np = full_seq.cpu().numpy()

            # ç”¨ä¸­ä½æ•°é¢„æµ‹ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
            median_pred = np.median(samples_np, axis=1)[:, :, 0]  # (B, T)
            true_norm = full_np[:, :, 0]

            # ç´¯è®¡è¯¯å·®
            delta2 = (median_pred - true_norm) ** 2 * mask_np
            delta1 = np.abs(median_pred - true_norm) * mask_np
            mse_sum += delta2.sum() * (scaler ** 2)
            mae_sum += delta1.sum() * scaler
            total_pts += mask_np.sum()

            all_preds.append(median_pred[mask_np] * scaler + mean_scaler)
            all_trues.append(true_norm[mask_np] * scaler + mean_scaler)

            all_generated_samples.append(samples_np)
            all_target.append(full_np)
            all_evalpoint.append(mask_np)
            all_observed_time.append(batch["timepoints"].numpy())

    if total_pts == 0:
        print("âš ï¸  æ— ä»»ä½• evaluation ç‚¹ï¼Œé€€å‡º")
        return

    # èšåˆæŒ‡æ ‡
    preds = np.concatenate(all_preds)
    trues = np.concatenate(all_trues)
    rmse = np.sqrt(mse_sum / total_pts)
    mae = mae_sum / total_pts
    r2 = r2_score(trues, preds)
    mape = np.mean(np.abs((preds - trues) / (trues + 1e-5))) * 100

    print("\nâœ… Evaluation finished.")
    print(f"   RMSE = {rmse:.4f}")
    print(f"   MAE  = {mae:.4f}")
    print(f"   RÂ²   = {r2:.4f}")
    print(f"   MAPE = {mape:.2f}%")

    if foldername:
        os.makedirs(foldername, exist_ok=True)

        with open(os.path.join(foldername, f"result_nsample{nsample}.pk"), "wb") as f:
            pickle.dump([rmse, mae, r2, mape], f)

        # â¬‡ï¸ æ‹¼æ¥æ‰€æœ‰æ ·æœ¬æ•°æ®å†ä¿å­˜
        all_generated_samples = np.concatenate(all_generated_samples, axis=0)  # (N, nsample, T, D)
        all_target = np.concatenate(all_target, axis=0)                        # (N, T, D)
        all_evalpoint = np.concatenate(all_evalpoint, axis=0)                 # (N, T)
        all_observed_time = np.concatenate(all_observed_time, axis=0)         # (N, T)

        with open(os.path.join(foldername, f"generated_outputs_nsample{nsample}.pk"), "wb") as f:
            pickle.dump([
                all_generated_samples,
                all_target,
                all_evalpoint,
                all_observed_time,
                scaler,
                mean_scaler
            ], f)

        with open(os.path.join(foldername, "metrics.txt"), "w") as f:
            f.write(f"RMSE: {rmse:.4f}\n")
            f.write(f"MAE : {mae:.4f}\n")
            f.write(f"R2  : {r2:.4f}\n")
            f.write(f"MAPE: {mape:.2f}%\n")

        print(f"\nğŸ“‚ æ‰€æœ‰æŒ‡æ ‡å’Œç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{foldername}/\n")


# def evaluate(
#     model,
#     test_loader,
#     nsample=100,
#     scaler=1.0,
#     mean_scaler=0.0,
#     foldername="",
# ):
#     """
#     è¯„ä¼°å‡½æ•°ï¼Œè¾“å‡º RMSE/MAE/RÂ²/MAPEï¼Œå¹¶å¯ä¿å­˜ç»“æœä¸ä¸­é—´æ ·æœ¬ã€‚
#     Args:
#       model        : å·²è®­ç»ƒå¥½çš„ CSDI_Speed æ¨¡å‹
#       test_loader  : æµ‹è¯•é›† DataLoaderï¼Œbatch ä¸­éœ€å« 'observed_data','observed_mask','gt_mask','timepoints'ï¼Œå¯é€‰ 'dates'
#       nsample      : æ¯æ¡åºåˆ—ç”Ÿæˆæ ·æœ¬æ•°
#       scaler       : åå½’ä¸€åŒ–ç”¨æ ‡å‡†å·®ï¼ˆscalarï¼‰
#       mean_scaler  : åå½’ä¸€åŒ–ç”¨å‡å€¼ï¼ˆscalarï¼‰
#       foldername   : éç©ºæ—¶ä¿å­˜ metrics.txt + generated_outputs_nsample{nsample}.pk + result_nsample{nsample}.pk
#     """
#     device = model.device
#     print(f"\nğŸ§ª Starting evaluation on {len(test_loader.dataset)} samples")
#     print(f"   â†’ ä½¿ç”¨ scaler={scaler:.6f}, mean={mean_scaler:.6f} åå½’ä¸€åŒ–\n")

#     all_preds, all_trues = [], []
#     mse_sum = mae_sum = total_pts = 0

#     model.eval()
#     with torch.no_grad():
#         for batch_i, batch in enumerate(tqdm(test_loader, desc="Evaluating"), start=1):
#             # æ¨åˆ° GPU
#             batch_gpu = {k: v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}
#             gm = batch_gpu["gt_mask"].float()     # (B, T, 1)
#             # æ„é€  eval mask åªè¯„ speed
#             eval_mask = (gm == 0).float()
#             eval_mask[:,:,1:] = 0.0
#             pts = int(eval_mask[:,:,0].sum().item())
#             if pts == 0:
#                 print(f"âš ï¸  Batch {batch_i} æ²¡æœ‰ eval ç‚¹ï¼Œè·³è¿‡")
#                 continue

#             # è°ƒç”¨ model.evaluate
#             samples, full_seq, _, _, _ = model.evaluate(batch, nsample)
#             # samples: (B,nsample,D,T) â†’ (B,nsample,T,D)
#             samples = samples.permute(0,1,3,2)
#             # è½¬ NumPy
#             samples_np = samples.cpu().numpy()             # (B, S, T, D)
#             full_np    = full_seq.cpu().numpy()            # (B, T, D)
#             mask_np    = eval_mask.cpu().numpy().squeeze(-1).astype(bool)  # (B, T)
#             time_np    = batch_gpu["timepoints"].cpu().numpy()            # (B, T)

#             B, S, T, D = samples_np.shape

#             # è®¡ç®—ä¸­ä½æ•°ï¼Œåœ¨å½’ä¸€åŒ–ç©ºé—´
#             med       = np.median(samples_np, axis=1)      # (B, T, D)
#             pred_norm = med[:,:,0]                         # (B, T)
#             true_norm = full_np[:,:,0]                     # (B, T)

#             if foldername:
#                 os.makedirs(foldername, exist_ok=True)
#                 with open(os.path.join(foldername, f"pred_true_values_batch{batch_i}.txt"), "w") as f:
#                     f.write("Predicted Values (normalized):\n")
#                     np.savetxt(f, pred_norm[mask_np], fmt='%.6f')
#                     f.write("\nTrue Values (normalized):\n")
#                     np.savetxt(f, true_norm[mask_np], fmt='%.6f')

#             # ç´¯è®¡ MSE/MAEï¼ˆå…ˆåœ¨å½’ä¸€åŒ–ç©ºé—´ç®—ï¼Œå†ä¹˜å› scaleï¼‰
#             delta2 = (pred_norm - true_norm)**2 * mask_np
#             delta1 = np.abs(pred_norm - true_norm) * mask_np
#             mse_sum   += delta2.sum() * (scaler**2)
#             mae_sum   += delta1.sum() * scaler
#             total_pts += mask_np.sum()

#             # æ”¶é›†æ‰€æœ‰ç‚¹ï¼Œç”¨äºå…¨å±€ RÂ²/MAPE
#             all_preds.append(pred_norm[mask_np] * scaler + mean_scaler)
#             all_trues.append(true_norm[mask_np] * scaler + mean_scaler)

#         if total_pts == 0:
#             print("âš ï¸  æ— ä»»ä½• evaluation ç‚¹ï¼Œé€€å‡º")
#             return

#     # å…¨å±€æŒ‡æ ‡
#     preds = np.concatenate(all_preds)
#     trues = np.concatenate(all_trues)
#     rmse  = np.sqrt(mse_sum / total_pts)
#     mae   = mae_sum / total_pts
#     r2    = r2_score(trues, preds)
#     mape  = np.mean(np.abs((preds - trues) / (trues + 1e-5))) * 100

#     print("\nâœ… Evaluation finished.")
#     print(f"   RMSE = {rmse:.4f}")
#     print(f"   MAE  = {mae:.4f}")
#     print(f"   RÂ²   = {r2:.4f}")
#     print(f"   MAPE = {mape:.2f}%\n")

#     if foldername:
#         os.makedirs(foldername, exist_ok=True)
#         # ä¿å­˜ç®€å•ç»“æœ
#         with open(os.path.join(foldername, f"result_nsample{nsample}.pk"), "wb") as f:
#             pickle.dump([rmse, mae, r2, mape], f)

#         # ä¿å­˜ç”Ÿæˆæ ·æœ¬ â€”â€” æ³¨æ„è¿™é‡Œå­˜å…¥ 6 é¡¹ï¼šsamples/full_seq/mask/time/scaler/mean_scaler
#         with open(os.path.join(foldername, f"generated_outputs_nsample{nsample}.pk"), "wb") as f:
#             pickle.dump([
#                 samples_np,   # (B, nsample, T, D)
#                 full_np,      # (B, T, D)
#                 mask_np,      # (B, T)
#                 time_np,      # (B, T)
#                 scaler,
#                 mean_scaler
#             ], f)

#         # ä¿å­˜ metrics.txt
#         with open(os.path.join(foldername, "metrics.txt"), "w") as f:
#             f.write(f"RMSE: {rmse:.4f}\n")
#             f.write(f"MAE : {mae:.4f}\n")
#             f.write(f"R2  : {r2:.4f}\n")
#             f.write(f"MAPE: {mape:.2f}%\n")
#         print(f"ğŸ“‚ Saved metrics & generated .pk to {foldername}")
# def evaluate(
#     model,
#     test_loader,
#     nsample=100,
#     scaler=1.0,
#     mean_scaler=0.0,
#     foldername="",
# ):
#     """
#     è¯„ä¼°å‡½æ•°ï¼Œè¾“å‡º RMSE/MAE/RÂ²/MAPEï¼Œå¹¶å¯ä¿å­˜ç»“æœä¸ä¸­é—´æ ·æœ¬ã€‚
#     Args:
#       model        : å·²è®­ç»ƒå¥½çš„ CSDI_Speed æ¨¡å‹
#       test_loader  : æµ‹è¯•é›† DataLoaderï¼Œbatch ä¸­éœ€å« 'observed_data','observed_mask','gt_mask','timepoints'ï¼Œå¯é€‰ 'dates'
#       nsample      : æ¯æ¡åºåˆ—ç”Ÿæˆæ ·æœ¬æ•°
#       scaler       : åå½’ä¸€åŒ–ç”¨æ ‡å‡†å·®ï¼ˆscalarï¼‰
#       mean_scaler  : åå½’ä¸€åŒ–ç”¨å‡å€¼ï¼ˆscalarï¼‰
#       foldername   : éç©ºæ—¶ä¿å­˜ metrics.txt + generated_outputs_nsample{nsample}.pk + result_nsample{nsample}.pk
#     """
#     device = model.device
#     print(f"\nğŸ§ª Starting evaluation on {len(test_loader.dataset)} samples")
#     print(f"   â†’ ä½¿ç”¨ scaler={scaler:.6f}, mean={mean_scaler:.6f} åå½’ä¸€åŒ–\n")

#     all_preds, all_trues = [], []
#     mse_sum = mae_sum = total_pts = 0

#     model.eval()
#     with torch.no_grad():
#         for batch_i, batch in enumerate(tqdm(test_loader, desc="Evaluating"), start=1):
#             # æ¨åˆ° GPU
#             batch_gpu = {k: v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}
#             gm = batch_gpu["gt_mask"].float()     # (B, T, 1)
#             # æ„é€  eval mask åªè¯„ speed
#             eval_mask = (gm == 0).float()
#             eval_mask[:,:,1:] = 0.0
#             pts = int(eval_mask[:,:,0].sum().item())
#             if pts == 0:
#                 print(f"âš ï¸  Batch{batch_i} æ²¡æœ‰ eval ç‚¹ï¼Œè·³è¿‡")
#                 continue

#             # è°ƒç”¨ model.evaluate
#             samples, full_seq, _, _, _ = model.evaluate(batch, nsample)
#             # samples: (B,nsample,D,T) â†’ (B,nsample,T,D)
#             samples = samples.permute(0,1,3,2)
#             # è½¬ NumPy
#             samples_np = samples.cpu().numpy()
#             full_np    = full_seq.cpu().numpy()
#             mask_np    = eval_mask.cpu().numpy().squeeze(-1).astype(bool)

#             B, S, T, D = samples_np.shape

#             # è®¡ç®—ä¸­ä½æ•°ï¼Œå½’ä¸€åŒ–ç©ºé—´
#             med = np.median(samples_np, axis=1)  # (B,T,D)
#             pred_norm = med[:,:,0]               # (B,T)
#             true_norm = full_np[:,:,0]           # (B,T)

#             # ç´¯è®¡ MSE/MAEï¼ˆåœ¨å½’ä¸€åŒ–ç©ºé—´å…ˆç®—ï¼Œå†ä¹˜å› scaleï¼‰
#             delta2 = (pred_norm - true_norm)**2 * mask_np
#             delta1 = np.abs(pred_norm - true_norm) * mask_np
#             mse_sum += delta2.sum() * (scaler**2)
#             mae_sum += delta1.sum() * scaler
#             total_pts += mask_np.sum()

#             # æ”¶é›†æ‰€æœ‰ç‚¹ç”¨äºå…¨å±€ RÂ²/MAPE
#             all_preds.append(pred_norm[mask_np] * scaler + mean_scaler)
#             all_trues.append(true_norm[mask_np] * scaler + mean_scaler)

#         if total_pts == 0:
#             print("âš ï¸  æ— ä»»ä½• evaluation ç‚¹ï¼Œé€€å‡º")
#             return

#     # å…¨å±€æŒ‡æ ‡
#     preds = np.concatenate(all_preds)
#     trues = np.concatenate(all_trues)
#     rmse = np.sqrt(mse_sum / total_pts)
#     mae  = mae_sum / total_pts
#     r2   = r2_score(trues, preds)
#     mape = np.mean(np.abs((preds - trues) / (trues + 1e-5))) * 100

#     print("\nâœ… Evaluation finished.")
#     print(f"   RMSE = {rmse:.4f}")
#     print(f"   MAE  = {mae:.4f}")
#     print(f"   RÂ²   = {r2:.4f}")
#     print(f"   MAPE = {mape:.2f}%\n")

#     # ç»“æœä¿å­˜
#     if foldername:
#         os.makedirs(foldername, exist_ok=True)
#         # ä¿å­˜ç®€å•ç»“æœ
#         res_path = os.path.join(foldername, f"result_nsample{nsample}.pk")
#         with open(res_path, "wb") as f:
#             pickle.dump([rmse, mae, r2, mape], f)
#         # ä¿å­˜ç”Ÿæˆæ ·æœ¬
#         gen_path = os.path.join(foldername, f"generated_outputs_nsample{nsample}.pk")
#         with open(gen_path, "wb") as f:
#             pickle.dump([samples_np, full_np, mask_np, scaler, mean_scaler], f)
#         # ä¿å­˜ metrics.txt
#         with open(os.path.join(foldername, "metrics.txt"), "w") as f:
#             f.write(f"RMSE: {rmse:.4f}\n")
#             f.write(f"MAE : {mae:.4f}\n")
#             f.write(f"R2  : {r2:.4f}\n")
#             f.write(f"MAPE: {mape:.2f}%\n")
#         print(f"ğŸ“‚ Saved metrics to {foldername}")